\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{framed}
\usepackage{url}

% Command for independent symbol
\def\ci{\perp}

% Command for not independent symbol
\def\notci{\not\perp}

% Packages for graphs and arrows
\usepackage{tikz}
\usetikzlibrary{arrows, calc, positioning, fit}

% Package for forcing figure placement
\usepackage{float}

\begin{document}

\title{CS 5526 -- Virginia Tech\\
	Homework 3}
\author{Brennon Bortz \\ (PID: brennon, Campus: Blacksburg)}
\date{15 April 2015}
\maketitle

Code for this assignment is available at \url{https://github.com/brennon/cs5526-hw3}.

\section*{Written Problems}

\begin{enumerate}

\item (20 points) Consider the unit hypersphere (with radius $r=1$). Inside the hypersphere inscribe a hypercube (i.e., the largest hypercube you can fit inside the hypersphere). An example in two dimensions is shown in Figure 6.12 (in the text). Answer the following questions:

\begin{enumerate}

\item Derive an expression for the volume of the inscribed hypercube for any given dimensionality $d$. Derive the expression for one, two, and three dimensions, and then generalize to higher dimensions.

\textbf{Solution:} For a hypersphere with radius $r=1$, $r$ will be half the diagonal of the largest hypercube that can be inscribed within this hypersphere. Thus, the length of the edge of the largest hypercube that can be inscribed within such a hypersphere is $2 \cdot \frac{1}{\sqrt{2}} = \frac{2}{\sqrt{2}}$. The volume of a hypercube with edge length $l$ is given as

\begin{equation}
\text{vol}(H_d(l)) = l^d
\end{equation}

Thus, the volume of the largest hypercube that can be inscribed within a hypersphere with radius $r=1$ in one dimension is

\begin{equation}
\text{vol}\left(H_1\left(\frac{2}{\sqrt{2}}\right)\right) = \left(\frac{2}{\sqrt{2}}\right)^1 = \frac{2}{\sqrt{2}}
\end{equation}

For two and three dimensions, we have

\begin{equation}
\text{vol}\left(H_2\left(\frac{2}{\sqrt{2}}\right)\right) = \left(\frac{2}{\sqrt{2}}\right)^2 = \frac{4}{2} = 2
\end{equation}

and

\begin{equation}
\text{vol}\left(H_3\left(\frac{2}{\sqrt{2}}\right)\right) = \left(\frac{2}{\sqrt{2}}\right)^3 = \frac{8}{2\sqrt{2}} = \frac{4}{\sqrt{2}}
\end{equation}

In general, for $d$ dimensions, we have

\begin{equation}
\text{vol}\left(H_d\left(\frac{2}{\sqrt{2}}\right)\right) = \left(\frac{2}{\sqrt{2}}\right)^d
\end{equation}

\item What happens to the ratio of the volume of the inscribed hypercube to the volume of the enclosing hyperspehere as $d \rightarrow \infty$? Again, give the ratio in one, two and three dimensions, and then generalize.

\textbf{Solution:} The volume of a hypersphere in $d$ dimensions with radius $r$, as given by Equation (6.4) in the text as

\begin{equation}
\text{vol}(S_d(r)) = K_d r^d = \left( \frac{\pi^\frac{d}{2}}{\Gamma \left( \frac{d}{2} + 1 \right)} \right) r^d
\end{equation}

As such, the ratio of the volume of the largest hypercube that can be inscribed within a hypersphere with radius $r=1$ in one dimension is

\begin{equation}
\frac{\text{vol}(H_1(\frac{2}{\sqrt{2}})}{\text{vol}(S_1(1))} = \frac{\left(\frac{2}{\sqrt{2}}\right)^1}{\left(\frac{\pi^{1/2}}{\Gamma(1/2  + 1)}\right)1^1} = \frac{\frac{2}{\sqrt{2}}}{\frac{\sqrt{\pi}}{\frac{\sqrt{\pi}}{2}}} = \frac{\frac{2}{\sqrt{2}}}{\frac{1}{\frac{1}{2}}} = \frac{\frac{2}{\sqrt{2}}}{2} = \frac{1}{\sqrt{2}}
\end{equation}

In two and three dimensions, we have

\begin{equation}
\frac{\text{vol}(H_2(\frac{2}{\sqrt{2}})}{\text{vol}(S_2(1))} = \frac{2}{\left(\frac{\pi^\frac{2}{2}}{\Gamma(\frac{2}{2}+1)}\right)1^2} = \frac{2}{\frac{\pi}{\Gamma(2)}} = \frac{2}{\frac{\pi}{1}} = \frac{2}{\pi}
\end{equation}

and

\begin{equation}
\frac{\text{vol}(H_3(\frac{2}{\sqrt{2}})}{\text{vol}(S_3(1))} = \frac{\frac{4}{\sqrt{2}}}{\left(\frac{\pi^\frac{3}{2}}{\Gamma(\frac{3}{2}+1)}\right)1^3} = \frac{\frac{4}{\sqrt{2}}}{\frac{\pi^\frac{3}{2}}{\frac{3}{4}\sqrt{\pi}}} = \frac{\frac{4}{\sqrt{2}}}{\frac{4}{3}\pi} = \frac{4}{\sqrt{2}} \cdot \frac{3}{4\pi} = \frac{12}{4\pi\sqrt{2}} = \frac{3}{\pi\sqrt{2}}
\end{equation}

In general, we have

\begin{equation}
\frac{\text{vol}(H_d(2/\sqrt{2}))}{\text{vol}(S_d(1))} = \frac{(2/\sqrt{2})^d}{\left(\frac{\pi^{d/2}}{\Gamma\left(\frac{d}{2}+1\right)}\right)1^d}
\end{equation}

As $d \rightarrow \infty$, the numerator also approaches $\inf$. The denominator, on the other hand, grows smaller and smaller, as $\Gamma$ is a factorial function. Thus, as $d \rightarrow \infty$, the ratio itself also approaches $\infty$.

\end{enumerate}

\item (20 points) Consider the data in Table 7.1 (in text). Define the kernel function as follows $K(\mathbf{x}_i, \mathbf{x}_j) = \Vert \mathbf{x}_i - \mathbf{x}_j \Vert^2$. Answer the following questions:

\begin{enumerate}
\item Compute the kernel matrix $\mathbf{K}$.

\textbf{Solution:}

\begin{eqnarray*}
K(\mathbf{x}_4, \mathbf{x}_1) &=& K(\mathbf{x}_1, \mathbf{x}_4) = \Vert (\begin{array}{cc}4 & 2.9 \end{array}) - (\begin{array}{cc}2.5 & 1 \end{array}) \Vert^2 \\
&=& \Vert (\begin{array}{cc} 1.5 & 1.9 \end{array}) \Vert^2 \\
&=& (\begin{array}{cc} 1.5 & 1.9 \end{array}) \left(\begin{array}{c} 1.5 \\ 1.9 \end{array}\right) \\
&=& 5.86
\end{eqnarray*}

\begin{eqnarray*}
K(\mathbf{x}_7, \mathbf{x}_1) &=& K(\mathbf{x}_1, \mathbf{x}_7) = \Vert (\begin{array}{cc}4 & 2.9 \end{array}) - (\begin{array}{cc}3.5 & 4 \end{array}) \Vert^2 \\
&=& \Vert (\begin{array}{cc} 0.5 & -1.1 \end{array}) \Vert^2 \\
&=& (\begin{array}{cc} 0.5 & -1.1 \end{array}) \left(\begin{array}{c} 0.5 \\ -1.1 \end{array}\right) \\
&=& 1.46
\end{eqnarray*}

\begin{eqnarray*}
K(\mathbf{x}_9, \mathbf{x}_1) &=& K(\mathbf{x}_1, \mathbf{x}_9) = \Vert (\begin{array}{cc}4 & 2.9 \end{array}) - (\begin{array}{cc} 2 & 2.1 \end{array}) \Vert^2 \\
&=& \Vert (\begin{array}{cc} 2 & 0.8 \end{array}) \Vert^2 \\
&=& (\begin{array}{cc} 2 & 0.8 \end{array}) \left(\begin{array}{c} 2 \\ 0.8 \end{array}\right) \\
&=& 4.64
\end{eqnarray*}

\begin{eqnarray*}
K(\mathbf{x}_7, \mathbf{x}_4) &=& K(\mathbf{x}_4, \mathbf{x}_7) = \Vert (\begin{array}{cc} 2.5 & 1 \end{array}) - (\begin{array}{cc} 3.5 & 4 \end{array}) \Vert^2 \\
&=& \Vert (\begin{array}{cc} -1 & -3 \end{array}) \Vert^2 \\
&=& (\begin{array}{cc} -1 & -3 \end{array}) \left(\begin{array}{c} -1 \\ -3 \end{array}\right) \\
&=& 10
\end{eqnarray*}

\begin{eqnarray*}
K(\mathbf{x}_9, \mathbf{x}_4) &=& K(\mathbf{x}_4, \mathbf{x}_9) = \Vert (\begin{array}{cc} 2.5 & 1 \end{array}) - (\begin{array}{cc} 2 & 2.1 \end{array}) \Vert^2 \\
&=& \Vert (\begin{array}{cc} 0.5 & -1.1 \end{array}) \Vert^2 \\
&=& (\begin{array}{cc} 0.5 & -1.1 \end{array}) \left(\begin{array}{c} 0.5 \\ -1.1 \end{array}\right) \\
&=& 1.46
\end{eqnarray*}

\begin{eqnarray*}
K(\mathbf{x}_9, \mathbf{x}_7) &=& K(\mathbf{x}_7, \mathbf{x}_9) = \Vert (\begin{array}{cc} 3.5 & 4 \end{array}) - (\begin{array}{cc} 2 & 2.1 \end{array}) \Vert^2 \\
&=& \Vert (\begin{array}{cc} 1.5 & 1.9 \end{array}) \Vert^2 \\
&=& (\begin{array}{cc} 1.5 & 1.9 \end{array}) \left(\begin{array}{c} 1.5 \\ 1.9 \end{array}\right) \\
&=& 5.86
\end{eqnarray*}

\begin{equation}
\mathbf{K} =
\left[\begin{array}{rrrr}
0	&	5.86	&	1.46	&	4.64 \\
5.86	&	0	&	10	&	1.46 \\
1.46	&	10	&	0	&	5.86 \\
4.64	&	1.46	&	5.86	&	0 \\
\end{array}\right]
\end{equation}

\item Find the first principal component.

By Equation 7.35 (in text)

\begin{equation}
\mathbf{Kc}=\eta_1\mathbf{c}
\end{equation}

where $\textbf{K}$ is the centered kernel matrix. The centered kernel matrix is given by

\begin{eqnarray}
\hat{\mathbf{K}} &=& \left( \mathbf{I} - \frac{1}{n} \mathbf{1}_{n \times n} \right) \mathbf{K} \left( \mathbf{I} - \frac{1}{n} \mathbf{1}_{n \times n} \right) \nonumber \\
&=&
\left[\begin{array}{rrrr}
   -2.32    & 2.2  & -2.2   & 2.32 \\
    2.2   & -5   & 5  & -2.2 \\
   -2.2    & 5  & -5   & 2.2 \\
    2.32   & -2.2   & 2.2  & -2.32 \\
\end{array}\right]
\end{eqnarray}

The eigenvalues of the centered kernel matrix are

\begin{eqnarray}
\lambda_1 &=& -12.4719 \\
\lambda_2 &=& -2.1681 \\
\lambda_3 &=& -0.4752 \\
\lambda_4 &=& 15.1152 \\
\end{eqnarray}

Thus, we have $\eta_1 = -12.4719$. The eigenvector corresponding to this eigenvalue gives $\mathbf{u}_1$, the first kernel principal component

\begin{equation}
\mathbf{u}_1 = 
\left(\begin{array}{r}
0.3463 \\
-0.6165 \\
0.6165 \\
-0.3463 \\
\end{array}\right)
\end{equation}

\end{enumerate}

\item Given the two points $\mathbf{x}_1 = (1,2)^\mathit{T}$, and $\mathbf{x}_2 = (2,1)^\mathit{T}$, use the kernel function

\begin{equation*}
K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i^\mathit{T}\mathbf{x}_j)^2
\end{equation*}

to find the kernel principal component, by solving the equation $\mathbf{Kc} = \eta_1\mathbf{c}$.

\textbf{Solution:}

\begin{equation}
K(\mathbf{x}_1, \mathbf{x}_1) = 
	\left[
		\left(
			\begin{array}{cc}
				1 & 2 \\
			\end{array}
		\right)
		\left(
			\begin{array}{c}
				1 \\
				2 \\
			\end{array}
		\right)
	\right]^2
	=
	5^2
	=
	25
\end{equation}

\begin{equation}
K(\mathbf{x}_2, \mathbf{x}_2) = 
	\left[
		\left(
			\begin{array}{cc}
				2 & 1 \\
			\end{array}
		\right)
		\left(
			\begin{array}{c}
				2 \\
				1 \\
			\end{array}
		\right)
	\right]^2
	=
	5^2
	=
	25
\end{equation}

\begin{equation}
K(\mathbf{x}_1, \mathbf{x}_2) = 
K(\mathbf{x}_2, \mathbf{x}_1) = 
	\left[
		\left(
			\begin{array}{cc}
				2 & 1 \\
			\end{array}
		\right)
		\left(
			\begin{array}{c}
				1 \\
				2 \\
			\end{array}
		\right)
	\right]^2
	=
	4^2
	=
	16
\end{equation}

Combining (17), (18), and (19)

\begin{equation}
\mathbf{K} =
\left[
	\begin{array}{cc}
		25 & 16 \\
		16 & 25 \\	
	\end{array}
\right]
\end{equation}

We now center the kernel matrix

\begin{eqnarray*}
\hat{\mathbf{K}} &=& \left( \mathbf{I} - \frac{1}{n} \mathbf{1}_{n \times n} \right) \mathbf{K} \left( \mathbf{I} - \frac{1}{n} \mathbf{1}_{n \times n} \right) \nonumber \\
&=& \left[
	\begin{array}{rr}
		4.5 & -4.5 \\
		-4.5 & 4.5 \\	
	\end{array}
\right]
\end{eqnarray*}

The eigenvalues of $\hat{\mathbf{K}}$ are

\begin{eqnarray}
\lambda_1 &=& 0 \\
\lambda_2 &=& 9
\end{eqnarray}

The first principal component, the eigenvector $\mathbf{c}$ (also $\mathbf{u}_1$) corresponding to $\eta_1$ (also $\lambda_1$), is

\begin{equation}
\mathbf{u}_1 = \left(
	\begin{array}{r}
		-0.7071 \\
		-0.7071 \\
	\end{array}
\right)
\end{equation}

\item (20 points) Consider the Prostate Cancer data from \url{http://statweb.stanford.edu/~tibs/ElemStatLearn/data.html}. Cast the linear regression problem over this dataset as one of linear programming, and solve it using any off-the-shelf LP solver. Present your results and analyze the performance of the regression.

\textbf{Solution:} We chose to consider linear regression in terms of minimizing the sum of all absolute values errors between actual values in the dataset and the value predicted for a given data point by linear regression

\begin{equation*}
\sum\limits_{i=1}^n | ax_i + b - y_i|
\end{equation*}

By introducing an error variable $e_i$ for each instance $x_i$, we can eliminate the absolute value in the sum of errors, and cast the problem of linear regression as the following linear program

\begin{table}[h]
\begin{center}
\begin{tabular}{lll}
Minimize & $\sum\limits_{i=1}^n e_i$ & \\
subject to & $Ax_i + b - y_i - e_i \leq 0$ & for $i=1,2,\ldots,n$ \\
 & $-(Ax_i + b - y_i) - e_i \leq 0$ & for $i=1,2,\ldots,n$
\end{tabular}
\end{center}
\end{table}

We used \verb|linprog| available in the MATLAB Optimization Toolbox for solving this problem. The code for this work is at the repository available at the link at the top of this document. We performed 10-fold cross-validation on the model generated by the solution to the linear programming problem. Below, we give the mean-squared error for the training ($MSE_{train}$) and testing ($MSE_{test}$) for each fold, as well as the average across all folds for each set type ($\overline{MSE}_{train}$ and $\overline{MSE}_{test}$.

\begin{table}[h]
\begin{center}
\begin{tabular}{lrr}
Fold &	$MSE_{train}$ &	$MSE_{test}$ \\ \hline
1 &	0.7723 &		0.2410 \\
2 &	0.6826 &		0.6293 \\
3 &	0.6609 &		0.8670 \\
4 &	0.6762 &		0.7364 \\
5 &	0.6535 &		0.9116 \\
6 &	0.6804 &		0.6354 \\
7 &	0.6538 &		1.1132 \\
8 &	0.6603 &		1.0048 \\
9 &	0.6291 &		1.1321 \\
10 &	0.6696 &		0.9369 \\
 & $\overline{MSE}_{train} = 0.6739$ &		 $\overline{MSE}_{test} = 0.8208$
\end{tabular}
\caption{Mean-squared error for the training ($MSE_{train}$) and testing ($MSE_{test}$) sets for each fold of 10-fold cross-validation.}
\end{center}
\label{tbl:q4-results}
\end{table}

\item (20 points) Consider the South African Heart Disease data from \url{http://statweb.stanford.edu/~tibs/ElemStatLearn/data.html}. Cast the linear classification problem over this dataset as one of linear programming, and solve it using any off-the-shelf LP solver. Present your results and analyze the performance of the classifier thus learnt.

\textbf{Solution:} As in SVMs, we seek the maximum-margin hyperplane that separates the two half-spaces containing the instances of each class. Should the two groups of instances be linearly separable, we only need to find the hyperplane such that $w^\mathit{T}x + b \geq 0$ for the first class and $w^\mathit{T}x + b \leq 0$ for the second class. However, if the two classes are not linearly separable, there will be no solution. Thus, we introduce a term to each inequality to account for possible misclassifications, so our inequalities become $w^\mathit{T}x + b \geq 1 - y$ and $w^\mathit{T}x + b \leq z - 1$. We now formulate a linear program to minimize these `misclassification' terms subject to these inequalities, with the additional restriction that our misclassification terms are greater than zero

\begin{table}[h]
\begin{center}
\begin{tabular}{lll}
Minimize & $\frac{1}{j} \sum\limits_{i=1}^j y_i + \frac{1}{k} \sum\limits_{i=1}^k z_i$ & \\
subject to & $-Aw - b - y \leq -1$ & \\
 & $Bx + b - z \leq -1$ & \\
 & $-y_i \leq 0$ & for $i = 1,2,\ldots,j$ \\
 & $-z_i \leq 0$ & for $i = 1,2,\ldots,k$ \\
\end{tabular}
\end{center}
\end{table}

where $A$ is the matrix of instances from the first class and $B$ is the matrix of instances from the second class. Note that we also minimize the sums of the means of the misclassification terms for each class, per \cite{mangasarian}.

We again perform 10-fold cross validation and report in-sample and out-of-sample accuracy for each fold, as well as overall mean accuracies.

\begin{table}[h]
\begin{center}
\begin{tabular}{lrr}
Fold &	$Accuracy_{train}$ &	$Accuracy_{test}$ \\ \hline
1  &	34.3750\% &	36.9565\% \\
2  &	33.7349\% &	42.5532\% \\
3  &	34.8558\% &	32.6087\% \\
4  &	34.6988\% &	34.0426\% \\
5  &	35.3365\% &	28.2609\% \\
6  &	34.8558\% &	32.6087\% \\
7  &	34.8558\% &	32.6087\% \\
8  &	33.4135\% &	45.6522\% \\
9  &	35.8173\% &	23.9130\% \\
10 &	34.3750\% &	36.9565\% \\
	& $\overline{Accuracy}_{train}=34.6318\%$ &	$\overline{Accuracy}_{train}=34.6161\%$ \\
\end{tabular}
\caption{Model accuracy for the training ($Accuracy_{train}$) and testing ($Accuracy_{test}$) for each fold of 10-fold cross-validation.}
\end{center}
\label{tbl:q5-results}
\end{table}

\end{enumerate}

\begin{thebibliography}{1}

\bibitem{mangasarian} Bennett, K.P., and Mangasarian, O.L. Robust linear programming discrimination of two linearly inseparable sets. \textit{Optimization Methods and Software}, \textit{1}(1). 23-34.

\end{thebibliography}

\end{document}


